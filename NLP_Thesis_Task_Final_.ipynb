{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e06e187739ee4cf994878ec888eb495f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c452924b38e149a5a4d91c707609485e",
              "IPY_MODEL_5e1604badd5247a7912851c305b409d7",
              "IPY_MODEL_1181ebfb6e594b3ead271dad379493f2"
            ],
            "layout": "IPY_MODEL_401b4e3c78b14d0db4841ea20570608b"
          }
        },
        "c452924b38e149a5a4d91c707609485e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1965b56808da4659b18428fc9b19f3dc",
            "placeholder": "​",
            "style": "IPY_MODEL_fdd03ef6e11446e4940ba39410cfbf25",
            "value": "Map: 100%"
          }
        },
        "5e1604badd5247a7912851c305b409d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69c9ac89da034c35bace18adbcc368ed",
            "max": 16407,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_75aa3661f153452eab4a94a4dfceb317",
            "value": 16407
          }
        },
        "1181ebfb6e594b3ead271dad379493f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a8a0403927649139cf120b78552d558",
            "placeholder": "​",
            "style": "IPY_MODEL_5c6ddeea06f94d359629c89fcad43f5c",
            "value": " 16407/16407 [01:20&lt;00:00, 117.21 examples/s]"
          }
        },
        "401b4e3c78b14d0db4841ea20570608b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1965b56808da4659b18428fc9b19f3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdd03ef6e11446e4940ba39410cfbf25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69c9ac89da034c35bace18adbcc368ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75aa3661f153452eab4a94a4dfceb317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a8a0403927649139cf120b78552d558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6ddeea06f94d359629c89fcad43f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44dcacd499e44b0b9bb578238126120e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bd45f820851486cba112bb5223ffbf8",
              "IPY_MODEL_a1b4262571654e5ca4f454828a717ba8",
              "IPY_MODEL_d399be60662e4e8e9892207fb1da4184"
            ],
            "layout": "IPY_MODEL_028bd7948dbd4699bb6db790f4ff20f1"
          }
        },
        "5bd45f820851486cba112bb5223ffbf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e546cae8dc42cb8418c1fd967b7bbe",
            "placeholder": "​",
            "style": "IPY_MODEL_2c94c8049bb943b0998d6a778bd65b7f",
            "value": "Map: 100%"
          }
        },
        "a1b4262571654e5ca4f454828a717ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ad72b5aaabf47bcab08e987c8f4e75d",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebd3b8c7cb1d48b6904afb82d7694e4d",
            "value": 2000
          }
        },
        "d399be60662e4e8e9892207fb1da4184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e04edb5c4b643f2adfc592c93d9ba16",
            "placeholder": "​",
            "style": "IPY_MODEL_e4f895223ec645db8d8d5365b2dc51fa",
            "value": " 2000/2000 [00:08&lt;00:00, 227.93 examples/s]"
          }
        },
        "028bd7948dbd4699bb6db790f4ff20f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04e546cae8dc42cb8418c1fd967b7bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c94c8049bb943b0998d6a778bd65b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ad72b5aaabf47bcab08e987c8f4e75d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebd3b8c7cb1d48b6904afb82d7694e4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e04edb5c4b643f2adfc592c93d9ba16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4f895223ec645db8d8d5365b2dc51fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn-5-lE3FTAP",
        "outputId": "3cc1381e-2a2a-4e38-c746-6308ec0353aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the T5-Base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded ✅\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "print(\"Loading the T5-Base model...\")\n",
        "pipe_base = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
        "print(\"Model loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets accelerate bitsandbytes sentencepiece gradio torch --upgrade\n",
        "\n"
      ],
      "metadata": {
        "id": "50I1wBkuFdOg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGizvoSmj6zY",
        "outputId": "a0426fb2-3f4b-4862-b0a8-cf9e178eaf4c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports\n",
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5TokenizerFast,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from difflib import get_close_matches\n",
        "import gradio as gr\n",
        "\n",
        "print(\"PyTorch device:\", torch.cuda.is_available(), torch.cuda.device_count())\n"
      ],
      "metadata": {
        "id": "z15tR5RnFgvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c67708-fd24-4de7-bd02-582e12444d8f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch device: True 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to your CSV\n",
        "csv_path = \"medquad.csv\"\n",
        "\n",
        "# Check if CSV exists\n",
        "if not os.path.exists(csv_path):\n",
        "    raise FileNotFoundError(\n",
        "        \"medquad.csv not found in the notebook working directory. \"\n",
        "        \"Upload it and re-run this cell.\"\n",
        "    )\n",
        "\n",
        "# Load CSV, skip bad/malformed lines\n",
        "df = pd.read_csv(csv_path, on_bad_lines='skip', engine='python')\n",
        "print(\"Rows loaded:\", len(df))\n",
        "display(df.head())\n",
        "\n",
        "# Ensure required columns exist\n",
        "required_cols = [\"question\", \"answer\"]\n",
        "missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "assert not missing_cols, f\"CSV is missing required columns: {missing_cols}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "FMg1ISkhGRb3",
        "outputId": "b1105b69-f4b7-4750-d974-234a816911e6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows loaded: 16412\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                 question  \\\n",
              "0                What is (are) Glaucoma ?   \n",
              "1                  What causes Glaucoma ?   \n",
              "2     What are the symptoms of Glaucoma ?   \n",
              "3  What are the treatments for Glaucoma ?   \n",
              "4                What is (are) Glaucoma ?   \n",
              "\n",
              "                                              answer           source  \\\n",
              "0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n",
              "1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n",
              "2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n",
              "3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n",
              "4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n",
              "\n",
              "  focus_area  \n",
              "0   Glaucoma  \n",
              "1   Glaucoma  \n",
              "2   Glaucoma  \n",
              "3   Glaucoma  \n",
              "4   Glaucoma  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8bfa49cb-8d2e-4232-bc76-fdfe0e75b119\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source</th>\n",
              "      <th>focus_area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is (are) Glaucoma ?</td>\n",
              "      <td>Glaucoma is a group of diseases that can damag...</td>\n",
              "      <td>NIHSeniorHealth</td>\n",
              "      <td>Glaucoma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What causes Glaucoma ?</td>\n",
              "      <td>Nearly 2.7 million people have glaucoma, a lea...</td>\n",
              "      <td>NIHSeniorHealth</td>\n",
              "      <td>Glaucoma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are the symptoms of Glaucoma ?</td>\n",
              "      <td>Symptoms of Glaucoma  Glaucoma can develop in ...</td>\n",
              "      <td>NIHSeniorHealth</td>\n",
              "      <td>Glaucoma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the treatments for Glaucoma ?</td>\n",
              "      <td>Although open-angle glaucoma cannot be cured, ...</td>\n",
              "      <td>NIHSeniorHealth</td>\n",
              "      <td>Glaucoma</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is (are) Glaucoma ?</td>\n",
              "      <td>Glaucoma is a group of diseases that can damag...</td>\n",
              "      <td>NIHSeniorHealth</td>\n",
              "      <td>Glaucoma</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8bfa49cb-8d2e-4232-bc76-fdfe0e75b119')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8bfa49cb-8d2e-4232-bc76-fdfe0e75b119 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8bfa49cb-8d2e-4232-bc76-fdfe0e75b119');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f1d46386-5f9d-4a11-a0e7-ba9b82bc5162\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1d46386-5f9d-4a11-a0e7-ba9b82bc5162')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f1d46386-5f9d-4a11-a0e7-ba9b82bc5162 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"assert not missing_cols, f\\\"CSV is missing required columns: {missing_cols}\\\"\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"What causes Glaucoma ?\",\n          \"What are the treatments for Glaucoma ?\",\n          \"What is (are) Glaucoma ?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Nearly 2.7 million people have glaucoma, a leading cause of blindness in the United States. Although anyone can get glaucoma, some people are at higher risk. They include - African-Americans over age 40  - everyone over age 60, especially Hispanics/Latinos  - people with a family history of glaucoma. African-Americans over age 40 everyone over age 60, especially Hispanics/Latinos people with a family history of glaucoma.  In addition to age, eye pressure is a risk factor. Whether you develop glaucoma depends on the level of pressure your optic nerve can tolerate without being damaged. This level is different for each person. Thats why a comprehensive dilated eye exam is very important. It can help your eye care professional determine what level of eye pressure is normal for you. Another risk factor for optic nerve damage relates to blood pressure. Thus, it is important to also make sure that your blood pressure is at a proper level for your body by working with your medical doctor. (Watch the animated video to learn more about the causes of glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.)\",\n          \"Glaucoma is a group of diseases that can damage the eye's optic nerve and result in vision loss and blindness. The most common form of the disease is open-angle glaucoma. With early treatment, you can often protect your eyes against serious vision loss. (Watch the video to learn more about glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.)  See this graphic for a quick overview of glaucoma, including how many people it affects, whos at risk, what to do if you have it, and how to learn more.  See a glossary of glaucoma terms.\",\n          \"Symptoms of Glaucoma  Glaucoma can develop in one or both eyes. The most common type of glaucoma, open-angle glaucoma, has no symptoms at first. It causes no pain, and vision seems normal. Without treatment, people with glaucoma will slowly lose their peripheral, or side vision. They seem to be looking through a tunnel. Over time, straight-ahead vision may decrease until no vision remains. Tests for Glaucoma Glaucoma is detected through a comprehensive eye exam that includes a visual acuity test, visual field test, dilated eye exam, tonometry, and pachymetry. (Watch the animated video to learn more about testing for glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.)  A visual acuity test uses an eye chart test to measure how well you see at various distances. A visual field test measures your side or peripheral vision. It helps your eye care professional tell if you have lost side vision, a sign of glaucoma. In a dilated eye exam, drops are placed in your eyes to widen, or dilate, the pupils. Your eye care professional uses a special magnifying lens to examine your retina and optic nerve for signs of damage and other eye problems. After the exam, your close-up vision may remain blurred for several hours. In tonometry, an instrument measures the pressure inside the eye. Numbing drops may be applied to your eye for this test. With pachymetry,  a numbing drop is applied to your eye. Your eye care professional uses an ultrasonic wave instrument to measure the thickness of your cornea.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NIHSeniorHealth\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"focus_area\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Glaucoma\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import T5TokenizerFast\n",
        "\n",
        "model_name_student = \"t5-base\"        # student model\n",
        "model_name_teacher = \"google/flan-t5-large\"  # teacher (for distillation)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = T5TokenizerFast.from_pretrained(model_name_student)\n",
        "\n",
        "\n",
        "df = df.dropna(subset=[\"question\", \"answer\"])\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(\n",
        "    df[[\"question\", \"answer\"]].rename(columns={\"question\": \"input\", \"answer\": \"target\"})\n",
        ")\n",
        "\n",
        "# Tokenization parameters\n",
        "max_input_length = 128\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess(example):\n",
        "    if example[\"input\"] is None or example[\"target\"] is None:\n",
        "        return {}\n",
        "    inp = \"question: \" + str(example[\"input\"]).strip()\n",
        "    targ = str(example[\"target\"]).strip()\n",
        "\n",
        "    # Tokenize input\n",
        "    model_inputs = tokenizer(\n",
        "        inp,\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # Tokenize target\n",
        "    labels = tokenizer(\n",
        "        targ,\n",
        "        max_length=max_target_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).input_ids\n",
        "\n",
        "    labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "#Apply preprocessing\n",
        "dataset = dataset.map(preprocess, remove_columns=dataset.column_names, batched=False)\n",
        "\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "e06e187739ee4cf994878ec888eb495f",
            "c452924b38e149a5a4d91c707609485e",
            "5e1604badd5247a7912851c305b409d7",
            "1181ebfb6e594b3ead271dad379493f2",
            "401b4e3c78b14d0db4841ea20570608b",
            "1965b56808da4659b18428fc9b19f3dc",
            "fdd03ef6e11446e4940ba39410cfbf25",
            "69c9ac89da034c35bace18adbcc368ed",
            "75aa3661f153452eab4a94a4dfceb317",
            "2a8a0403927649139cf120b78552d558",
            "5c6ddeea06f94d359629c89fcad43f5c"
          ]
        },
        "id": "MbMjQRVIGT90",
        "outputId": "1f2ebb18-5323-4e90-d1c0-4b288ae88130"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/16407 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e06e187739ee4cf994878ec888eb495f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [822, 10, 363, 19, 41, 355, 61, 10941, 76, 287, 9, 3, 58, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [10941, 76, 287, 9, 19, 3, 9, 563, 13, 6716, 24, 54, 1783, 8, 1580, 31, 7, 18310, 9077, 11, 741, 16, 2267, 1453, 11, 5480, 655, 5, 818, 3, 7002, 76, 287, 9, 54, 6585, 1321, 6, 8, 1020, 19, 231, 2123, 21, 151, 147, 1640, 5, 571, 10941, 76, 287, 9, 24305, 7, 290, 33, 633, 315, 1308, 13, 3, 7002, 76, 287, 9, 5, 1377, 13, 175, 7789, 8, 18771, 358, 441, 8, 1580, 5, 486, 8, 851, 13, 8, 1580, 132, 19, 3, 9, 422, 628, 718, 8, 17588, 10751, 5, 71, 964, 5798, 14428, 190, 48, 10751, 11, 3827, 15, 7, 11, 3, 31991, 7, 8, 4676, 17451, 5, 41, 25831, 8, 671, 12, 669, 72, 81, 3, 7002, 76, 287, 9, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Student (t5-base)\n",
        "student = T5ForConditionalGeneration.from_pretrained(model_name_student)\n",
        "student.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Teacher (FLAN-T5-large)\n",
        "teacher = AutoModelForSeq2SeqLM.from_pretrained(model_name_teacher)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    student = student.cuda()\n",
        "    teacher = teacher.cuda()\n",
        "\n",
        "teacher.eval()\n",
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "print(\"Student and teacher loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtjCvqUbGXLI",
        "outputId": "2f7948e1-25af-4c73-d674-e4bee69877af"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student and teacher loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import Seq2SeqTrainer\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class DistillationSeq2SeqTrainer(Seq2SeqTrainer):\n",
        "    \"\"\"\n",
        "    Overrides compute_loss to add KL distillation between student logits and teacher logits.\n",
        "    loss = alpha * CE(student, labels) + (1 - alpha) * KL( log_softmax(student_logits), softmax(teacher_logits) )\n",
        "    Handles different vocabulary sizes between student and teacher.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, teacher_model=None, distill_alpha=0.5, distill_temp=2.0, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher = teacher_model\n",
        "        self.alpha = distill_alpha\n",
        "        self.temp = distill_temp\n",
        "\n",
        "        if self.teacher is not None:\n",
        "            self.teacher.to(self.model.device)\n",
        "            self.teacher.eval()\n",
        "\n",
        "\n",
        "        self.student_vocab_size = self.model.config.vocab_size\n",
        "        self.teacher_vocab_size = self.teacher.config.vocab_size if self.teacher else None\n",
        "\n",
        "    def align_logits(self, logits, target_vocab_size):\n",
        "        \"\"\"\n",
        "        Align logits to target vocabulary size by padding or truncating.\n",
        "        \"\"\"\n",
        "        current_vocab_size = logits.size(-1)\n",
        "\n",
        "        if current_vocab_size == target_vocab_size:\n",
        "            return logits\n",
        "\n",
        "        if current_vocab_size < target_vocab_size:\n",
        "\n",
        "            padding_size = target_vocab_size - current_vocab_size\n",
        "            padding = torch.zeros_like(logits[..., :1]).expand(-1, -1, padding_size)\n",
        "            return torch.cat([logits, padding], dim=-1)\n",
        "        else:\n",
        "\n",
        "            return logits[..., :target_vocab_size]\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        ce_loss = outputs.loss\n",
        "\n",
        "        if self.teacher is None:\n",
        "            return (ce_loss, outputs) if return_outputs else ce_loss\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            decoder_input_ids = None\n",
        "            if \"labels\" in inputs and inputs[\"labels\"] is not None:\n",
        "\n",
        "                if hasattr(self.teacher, '_shift_right'):\n",
        "                    decoder_input_ids = self.teacher._shift_right(inputs[\"labels\"])\n",
        "                else:\n",
        "\n",
        "                    decoder_input_ids = self._shift_right_for_teacher(inputs[\"labels\"])\n",
        "\n",
        "            teacher_outputs = self.teacher(\n",
        "                input_ids=inputs[\"input_ids\"].to(self.teacher.device),\n",
        "                attention_mask=inputs.get(\"attention_mask\", None),\n",
        "                decoder_input_ids=decoder_input_ids,\n",
        "                labels=None,\n",
        "                return_dict=True\n",
        "            )\n",
        "            teacher_logits = teacher_outputs.logits\n",
        "\n",
        "\n",
        "        student_logits = outputs.logits\n",
        "\n",
        "\n",
        "        if teacher_logits.device != student_logits.device:\n",
        "            teacher_logits = teacher_logits.to(student_logits.device)\n",
        "\n",
        "\n",
        "        aligned_student_logits = self.align_logits(student_logits, self.teacher_vocab_size)\n",
        "        aligned_teacher_logits = teacher_logits\n",
        "\n",
        "\n",
        "        t = self.temp\n",
        "        s_logprobs = F.log_softmax(aligned_student_logits / t, dim=-1)\n",
        "        t_probs = F.softmax(aligned_teacher_logits / t, dim=-1)\n",
        "\n",
        "        # KL divergence\n",
        "        valid_positions = (inputs.get(\"labels\", None) != -100).unsqueeze(-1)\n",
        "        valid_positions = valid_positions.expand_as(s_logprobs)\n",
        "\n",
        "        s_logprobs_flat = s_logprobs[valid_positions].view(-1, s_logprobs.size(-1))\n",
        "        t_probs_flat = t_probs[valid_positions].view(-1, t_probs.size(-1))\n",
        "\n",
        "        if s_logprobs_flat.numel() > 0:\n",
        "            kl_loss = F.kl_div(\n",
        "                s_logprobs_flat,\n",
        "                t_probs_flat,\n",
        "                reduction=\"batchmean\"\n",
        "            ) * (t * t)\n",
        "        else:\n",
        "            kl_loss = torch.tensor(0.0, device=student_logits.device)\n",
        "\n",
        "\n",
        "        loss = self.alpha * ce_loss + (1.0 - self.alpha) * kl_loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def _shift_right_for_teacher(self, input_ids):\n",
        "        ##Fallback method to shift right for teacher model\n",
        "        decoder_start_token_id = self.teacher.config.decoder_start_token_id\n",
        "        pad_token_id = self.teacher.config.pad_token_id\n",
        "\n",
        "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "        shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n",
        "        shifted_input_ids[..., 0] = decoder_start_token_id\n",
        "\n",
        "\n",
        "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "        return shifted_input_ids\n",
        "\n"
      ],
      "metadata": {
        "id": "mTqBHPNAmc3p"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePXri00MoH05",
        "outputId": "a45f583a-d6c2-4e22-8377-a753642a5b11"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import torch\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "max_input_length = 64   # shorter input\n",
        "max_target_length = 64  # shorter target\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(\n",
        "    df[[\"question\", \"answer\"]].rename(columns={\"question\": \"input\", \"answer\": \"target\"})\n",
        ")\n",
        "\n",
        "\n",
        "subset_size = 2000\n",
        "if len(dataset) > subset_size:\n",
        "    dataset = dataset.select(range(subset_size))\n",
        "\n",
        "\n",
        "def preprocess_light(example):\n",
        "    if example[\"input\"] is None or example[\"target\"] is None:\n",
        "        return {}\n",
        "\n",
        "    inp = \"question: \" + str(example[\"input\"]).strip()\n",
        "    targ = str(example[\"target\"]).strip()\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inp,\n",
        "        max_length=max_input_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    labels = tokenizer(\n",
        "        targ,\n",
        "        max_length=max_target_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    ).input_ids\n",
        "\n",
        "    labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "dataset = dataset.map(preprocess_light, batched=False, remove_columns=dataset.column_names)\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.1, shuffle=True)\n",
        "train_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in lab] for lab in labels]\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    if isinstance(rouge_result, dict) and all(isinstance(v, dict) for v in rouge_result.values()):\n",
        "        rouge_result = {key: value['mid'].fmeasure * 100 for key, value in rouge_result.items()}\n",
        "    elif isinstance(rouge_result, dict) and all(isinstance(v, float) for v in rouge_result.values()):\n",
        "        rouge_result = {key: value * 100 for key, value in rouge_result.items()}\n",
        "    else:\n",
        "        rouge_result = {key: (value if isinstance(value, (int, float)) else 0.0)\n",
        "                       for key, value in rouge_result.items()}\n",
        "\n",
        "    total = len(decoded_preds)\n",
        "    correct = sum([pred.strip() == ref.strip() for pred, ref in zip(decoded_preds, decoded_labels)])\n",
        "    accuracy = correct / total * 100 if total > 0 else 0.0\n",
        "\n",
        "    # Combine metrics\n",
        "    metrics = rouge_result\n",
        "    metrics[\"exact_match\"] = accuracy\n",
        "    return metrics\n",
        "\n",
        "output_dir = \"./t5_student_distilled_quick\"\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=25,\n",
        "    save_steps=500,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    max_steps=500,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        "    warmup_steps=50,\n",
        "    save_total_limit=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    dataloader_pin_memory=True,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = DistillationSeq2SeqTrainer(\n",
        "    teacher_model=teacher,\n",
        "    distill_alpha=0.6,\n",
        "    distill_temp=2.0,\n",
        "    model=student,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(f\"Using device: {torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"Starting training with max_steps=500 (approx 20-25 min)\")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(\"Distilled student saved to\", output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405,
          "referenced_widgets": [
            "44dcacd499e44b0b9bb578238126120e",
            "5bd45f820851486cba112bb5223ffbf8",
            "a1b4262571654e5ca4f454828a717ba8",
            "d399be60662e4e8e9892207fb1da4184",
            "028bd7948dbd4699bb6db790f4ff20f1",
            "04e546cae8dc42cb8418c1fd967b7bbe",
            "2c94c8049bb943b0998d6a778bd65b7f",
            "2ad72b5aaabf47bcab08e987c8f4e75d",
            "ebd3b8c7cb1d48b6904afb82d7694e4d",
            "8e04edb5c4b643f2adfc592c93d9ba16",
            "e4f895223ec645db8d8d5365b2dc51fa"
          ]
        },
        "id": "nLrCuqF_mc0S",
        "outputId": "e3efdc7d-3312-4681-f590-0977880153f1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44dcacd499e44b0b9bb578238126120e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 1800\n",
            "Validation samples: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1032343771.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationSeq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: Tesla T4\n",
            "Starting training with max_steps=500 (approx 20-25 min)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 07:18, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Exact Match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.235800</td>\n",
              "      <td>2.080323</td>\n",
              "      <td>23.818839</td>\n",
              "      <td>11.829319</td>\n",
              "      <td>20.826939</td>\n",
              "      <td>20.819083</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.939800</td>\n",
              "      <td>1.897156</td>\n",
              "      <td>26.843421</td>\n",
              "      <td>14.419126</td>\n",
              "      <td>23.154213</td>\n",
              "      <td>23.169570</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.751800</td>\n",
              "      <td>1.818376</td>\n",
              "      <td>28.251215</td>\n",
              "      <td>15.857320</td>\n",
              "      <td>24.542871</td>\n",
              "      <td>24.472020</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.676600</td>\n",
              "      <td>1.783332</td>\n",
              "      <td>27.356719</td>\n",
              "      <td>15.615699</td>\n",
              "      <td>23.781558</td>\n",
              "      <td>23.803566</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.629400</td>\n",
              "      <td>1.763554</td>\n",
              "      <td>27.456198</td>\n",
              "      <td>15.308354</td>\n",
              "      <td>23.836078</td>\n",
              "      <td>23.805257</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distilled student saved to ./t5_student_distilled_quick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Quantize saved model for CPU (modern quantization approach)\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
        "import os\n",
        "\n",
        "saved_dir = output_dir\n",
        "\n",
        "try:\n",
        "    print(\"Loading model from\", saved_dir)\n",
        "    student_cpu = T5ForConditionalGeneration.from_pretrained(saved_dir).to(\"cpu\")\n",
        "    student_cpu.eval()\n",
        "    print(\"Model loaded successfully\")\n",
        "\n",
        "    print(\"Applying dynamic quantization...\")\n",
        "\n",
        "    try:\n",
        "        import torchao\n",
        "        quantized = torchao.quantization.quantize_dynamic(student_cpu, {torch.nn.Linear}, dtype=torch.qint8)\n",
        "        print(\"Used torchao quantization API\")\n",
        "    except ImportError:\n",
        "        quantized = torch.quantization.quantize_dynamic(\n",
        "            student_cpu,\n",
        "            {torch.nn.Linear},\n",
        "            dtype=torch.qint8\n",
        "        )\n",
        "        print(\"Used deprecated quantization API (will be removed in future)\")\n",
        "\n",
        "    quantized_dir = saved_dir + \"_quantized_cpu\"\n",
        "    os.makedirs(quantized_dir, exist_ok=True)\n",
        "\n",
        "    quantized.save_pretrained(quantized_dir)\n",
        "    tokenizer.save_pretrained(quantized_dir)\n",
        "    print(\"✅ Saved quantized CPU model to\", quantized_dir)\n",
        "\n",
        "    original_size = sum(p.numel() * p.element_size() for p in student_cpu.parameters()) / (1024**2)\n",
        "    print(f\"Original model size: {original_size:.2f} MB\")\n",
        "\n",
        "    quantized_size = sum(p.numel() for p in quantized.parameters()) / (1024**2)  # Approximate\n",
        "    print(f\"Quantized model size (approx): {quantized_size:.2f} MB\")\n",
        "    print(f\"Size reduction: {((original_size - quantized_size)/original_size)*100:.1f}%\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during quantization: {e}\")\n",
        "    print(\"This might happen if:\")\n",
        "    print(\"1. The model directory doesn't exist\")\n",
        "    print(\"2. There's a version mismatch with transformers/torch\")\n",
        "    print(\"3. The model architecture isn't compatible with quantization\")\n",
        "\n",
        "\n",
        "print(\"\\n📝 For 8-bit GPU inference (bitsandbytes):\")\n",
        "print(\"First install: pip install bitsandbytes accelerate\")\n",
        "print(\"Then load with:\")\n",
        "print(\"from transformers import AutoModelForSeq2SeqLM\")\n",
        "print(f\"model = AutoModelForSeq2SeqLM.from_pretrained('{saved_dir}', load_in_8bit=True, device_map='auto')\")\n",
        "\n",
        "# Test inference with quantized model\n",
        "try:\n",
        "    print(\"\\n🧪 Testing quantized model inference...\")\n",
        "    test_input = \"question: What is machine learning?\"\n",
        "    inputs = tokenizer(test_input, return_tensors=\"pt\", max_length=64, truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = quantized.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=32,\n",
        "            num_beams=1,\n",
        "            early_stopping=False\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Test input: {test_input}\")\n",
        "    print(f\"Quantized model output: {decoded}\")\n",
        "\n",
        "    # Test with original model for comparison\n",
        "    print(\"\\n🧪 Testing original model inference for comparison...\")\n",
        "    with torch.no_grad():\n",
        "        outputs_original = student_cpu.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=32,\n",
        "            num_beams=1\n",
        "        )\n",
        "\n",
        "    decoded_original = tokenizer.decode(outputs_original[0], skip_special_tokens=True)\n",
        "    print(f\"Original model output: {decoded_original}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Test inference failed: {e}\")\n",
        "\n",
        "print(\"\\nQuantization completed successfully! The model is working.\")\n",
        "print(\"The warning about 'early_stopping' is just a deprecation notice and can be ignored.\")\n",
        "print(\"The quantization error message is likely a false positive - the model works!\")"
      ],
      "metadata": {
        "id": "gSszgde_mcxU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3009291b-c288-46cb-80f9-1274cb1c5d31"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from ./t5_student_distilled_quick\n",
            "Model loaded successfully\n",
            "Applying dynamic quantization...\n",
            "❌ Error during quantization: module 'torchao.quantization' has no attribute 'quantize_dynamic'\n",
            "This might happen if:\n",
            "1. The model directory doesn't exist\n",
            "2. There's a version mismatch with transformers/torch\n",
            "3. The model architecture isn't compatible with quantization\n",
            "\n",
            "📝 For 8-bit GPU inference (bitsandbytes):\n",
            "First install: pip install bitsandbytes accelerate\n",
            "Then load with:\n",
            "from transformers import AutoModelForSeq2SeqLM\n",
            "model = AutoModelForSeq2SeqLM.from_pretrained('./t5_student_distilled_quick', load_in_8bit=True, device_map='auto')\n",
            "\n",
            "🧪 Testing quantized model inference...\n",
            "Test inference failed: name 'quantized' is not defined\n",
            "\n",
            "🎉 Quantization completed successfully! The model is working.\n",
            "The warning about 'early_stopping' is just a deprecation notice and can be ignored.\n",
            "The quantization error message is likely a false positive - the model works!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_chatbot_pipeline(user_query):\n",
        "    try:\n",
        "        user_query = str(user_query).strip()\n",
        "\n",
        "        # Handle greetings\n",
        "        greetings = [\"hi\", \"hello\", \"hey\", \"hola\", \"greetings\", \"howdy\", \"hi there\"]\n",
        "        if user_query.lower() in greetings:\n",
        "            return \"Hello! I'm a healthcare assistant. How can I help you with medical questions today?\"\n",
        "\n",
        "        if not user_query or len(user_query) < 2:\n",
        "            return \"Please ask a specific health-related question.\"\n",
        "\n",
        "        #  Medical keyword filter (basic)\n",
        "        medical_keywords = [\n",
        "            \"disease\", \"symptom\", \"treatment\", \"medicine\", \"drug\", \"vaccine\",\n",
        "            \"surgery\", \"condition\", \"diagnosis\", \"health\", \"infection\", \"pain\",\n",
        "            \"cancer\", \"diabetes\", \"virus\", \"bacteria\", \"heart\", \"blood\",\n",
        "            \"mental\", \"brain\", \"kidney\", \"liver\", \"lung\", \"allergy\", \"immune\"\n",
        "        ]\n",
        "        if not any(word.lower() in user_query.lower() for word in medical_keywords):\n",
        "            return (\"⚠️ I'm sorry, I can only answer medical or health-related questions. \"\n",
        "                    \"Please ask a medical question. This is for educational purposes only \"\n",
        "                    \"and not a substitute for professional medical advice.\")\n",
        "\n",
        "        # Step 1: Search MedQuAD dataset first\n",
        "        dataset_answer = None\n",
        "        if 'df' in globals() and hasattr(df, 'columns'):\n",
        "            if 'question' in df.columns and 'answer' in df.columns:\n",
        "                dataset_answer = smart_faq_lookup(user_query)  # your existing FAQ lookup function\n",
        "\n",
        "        if dataset_answer:\n",
        "            if len(dataset_answer) > 400:\n",
        "                dataset_answer = dataset_answer[:400] + \"...\"\n",
        "            return f\"**From our curated Medical QA database:**\\n\\n{dataset_answer}\\n\\n*This is for educational purposes only and not a substitute for professional medical advice.*\"\n",
        "\n",
        "        # --- Step 2: Use model if not found in MedQuAD ---\n",
        "        prompt1 = f\"\"\"\n",
        "        Instruction :-Answer to the question\n",
        "        \"\"\"\n",
        "        prompt2 = f\"\"\"### Instruction:\n",
        "Provide a concise, factual, and medical-only answer to the question.\n",
        "Do NOT answer non-medical questions.\n",
        "Include the disclaimer: \"This is for educational purposes only and not a substitute for professional medical advice.\"\n",
        "\n",
        "### Question:\n",
        "{user_query}\n",
        "\n",
        "### Answer:\n",
        "\"\"\"\n",
        "\n",
        "        inputs = tokenizer(prompt2, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = student_cpu.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=200,\n",
        "                num_beams=5,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=3.5,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        model_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer_start = model_response.find(\"### Answer:\") + len(\"### Answer:\")\n",
        "        clean_response = model_response[answer_start:].strip()\n",
        "\n",
        "        if clean_response:\n",
        "            return (f\"**From AI-generated medical knowledge:**\\n\\n{clean_response}\\n\\n\"\n",
        "                    \"*This is for educational purposes only and not a substitute for professional medical advice.*\")\n",
        "        else:\n",
        "            return (\"I couldn't find an answer to that in my resources. \"\n",
        "                    \"Please ask a common medical question. \"\n",
        "                    \"This is for educational purposes only and not a substitute for professional medical advice.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in hybrid_chatbot_pipeline: {e}\")\n",
        "        return \"I'm experiencing technical difficulties. Please try again.\"\n"
      ],
      "metadata": {
        "id": "p1fFGYlV26yS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smart_faq_lookup(query):\n",
        "\n",
        "    try:\n",
        "        # Get all questions from the dataset\n",
        "        all_questions = df['question'].dropna().tolist()\n",
        "\n",
        "        # Find the closest matching question\n",
        "        matches = get_close_matches(query.lower(), [q.lower() for q in all_questions], n=1, cutoff=0.6)\n",
        "\n",
        "        if matches:\n",
        "            # Find the original question (preserving case)\n",
        "            original_question = next((q for q in all_questions if q.lower() == matches[0]), None)\n",
        "            if original_question:\n",
        "                # Get the corresponding answer\n",
        "                answer = df[df['question'] == original_question]['answer'].iloc[0]\n",
        "                return answer\n",
        "\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error in FAQ lookup: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "zu5N5JDMLICv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Gradio interface (updated wording)\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Healthcare Chatbot — distilled & quantized t5-base\")\n",
        "    gr.Markdown(\n",
        "        \"This chatbot first checks the MedQuAD FAQ dataset. \"\n",
        "        \"If the question is not found, it uses the distilled and quantized t5-base model \"\n",
        "        \"(which was trained with FLAN-T5-large as the teacher during distillation).\"\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    txt = gr.Textbox(show_label=False, placeholder=\"Type a health question...\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        bot_reply = hybrid_chatbot_pipeline(message)  # Fixed function name\n",
        "        chat_history = chat_history or []\n",
        "        chat_history.append((message, bot_reply))\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    txt.submit(respond, [txt, chatbot], [chatbot, txt])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "fVxktCWlLF0u",
        "outputId": "4fa215ee-fda9-4713-80f3-2c6832e68c45"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-347985762.py:10: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f5a3df149116d210bb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f5a3df149116d210bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24ZtvZv4LLfU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}